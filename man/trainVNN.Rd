% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bridge-julia.R
\name{trainVNN}
\alias{trainVNN}
\title{Train a VNN Model via Julia}
\usage{
trainVNN(
  se,
  architecture,
  label_col,
  assay_name = NULL,
  task = c("classification", "regression"),
  epochs = 100L,
  learning_rate = 0.001,
  batch_size = 64L,
  val_fraction = 0.2,
  l1_lambda = 1e-04,
  patience = 0L,
  min_delta = 1e-04,
  seed = 42L,
  verbose = TRUE
)
}
\arguments{
\item{se}{A \code{SummarizedExperiment} with the expression assay.}

\item{architecture}{A \code{VNNArchitecture} object.}

\item{label_col}{Character. Column in \code{colData(se)} to use as the
response variable.}

\item{assay_name}{Character. Name of the assay to use. Default "counts" or
first available.}

\item{task}{One of "classification", "regression".}

\item{epochs}{Integer. Number of training epochs. Default 100.}

\item{learning_rate}{Numeric. Adam optimizer LR. Default 1e-3.}

\item{batch_size}{Integer. Mini-batch size. Default 64.}

\item{val_fraction}{Numeric in (0,1). Fraction held out for validation.}

\item{l1_lambda}{Numeric. L1 penalty on masked weights for sparsity.}

\item{patience}{Integer. Stop training if validation loss does not improve
for this many consecutive epochs. Set to 0 to disable early stopping.
Default 0 (disabled).}

\item{min_delta}{Numeric. Minimum improvement in validation loss to be
considered progress. Default 1e-4.}

\item{seed}{Integer. Random seed for reproducibility.}

\item{verbose}{Logical. Print per-epoch progress? Default TRUE.}
}
\value{
A \code{VNNModel} object.
}
\description{
Serializes the expression matrix, label vector, and architecture masks to
Julia, then calls the Lux.jl training loop.
}
